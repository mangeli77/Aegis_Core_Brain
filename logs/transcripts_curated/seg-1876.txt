[2025-07-06T00:00:1875Z] (neutral)  [#?? let's do the following now #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) ? 1. UpdatedÊvoice-cognition.js [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral)  [#hereõs a safer version that #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) * Auto-stops after 10 seconds [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) * Cleans up logging [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) * Keeps all emotional/speaker logic intact [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral)  [#?? let's do the following now #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) ? 1. UpdatedÊvoice-cognition.js [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral)  [#hereõs a safer version that #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) * Auto-stops after 10 seconds [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) * Cleans up logging [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) * Keeps all emotional/speaker logic intact [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) ${MODEL_PATH}`); process.exit(1); } // Load model const model = new Model(MODEL_PATH); const recognizer = new KaldiRecognizer(model, SAMPLE_RATE); recognizer.setWords(true); // Mic config const micInstance = mic({ rate: String(SAMPLE_RATE), channels: '1', debug: false, exitOnSilence: 6, }); const micInputStream = micInstance.getAudioStream(); // Voice pipeline micInputStream.on('data', async (data) => { if (recognizer.acceptWaveform(data)) { const result = JSON.parse(recognizer.result()); const transcript = result.text.trim(); if (!transcript) return; const speakerVerified = await verifySpeaker(data); const emotion = await analyzeEmotion(data); const tags = []; if (speakerVerified) tags.push('[voice: verified]'); if (emotion) tags.push(`[emotion: ${emotion}]`); const taggedTranscript = `${tags.join(' ')} ${transcript}`; console.log(`?? ${taggedTranscript}`); await handleInput(taggedTranscript); } }); micInputStream.on('error', (err) => { console.error('?? Mic error:', err.message); }); micInputStream.on('startComplete', () => { console.log('?? Voice cognition pipeline activated (Vosk + SpeechBrain)'); }); // Auto-stop after 10 seconds for dev setTimeout(() => { console.log('?? Mic auto-stopping after 10 seconds...'); micInstance.stop(); }, 10000); micInstance.start(); [#// voice/voice-cognition.js import fs from 'fs'; import mic from 'mic'; import { model, kaldirecognizer } from 'vosk'; import { analyzeemotion, verifyspeaker } from '../utils/speechbrain.js'; import { handleinput } from '../scripts/soul-core.js'; // replace with correct path if needed const model_path = '/users/aegis/public/drop box/vosk-model-en-us-0.22'; const sample_rate = 16000; if (!fs.existssync(model_path)) { console.error(`? vosk model not found at #seg-1876]
[2025-07-06T00:00:1875Z] (neutral)  [#?? let's do the following now #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) ? 1. UpdatedÊvoice-cognition.js [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral)  [#hereõs a safer version that #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) * Auto-stops after 10 seconds [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) * Cleans up logging [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) * Keeps all emotional/speaker logic intact [#system #seg-1876]
[2025-07-06T00:00:1875Z] (neutral) ${MODEL_PATH}`); process.exit(1); } // Load model const model = new Model(MODEL_PATH); const recognizer = new KaldiRecognizer(model, SAMPLE_RATE); recognizer.setWords(true); // Mic config const micInstance = mic({ rate: String(SAMPLE_RATE), channels: '1', debug: false, exitOnSilence: 6, }); const micInputStream = micInstance.getAudioStream(); // Voice pipeline micInputStream.on('data', async (data) => { if (recognizer.acceptWaveform(data)) { const result = JSON.parse(recognizer.result()); const transcript = result.text.trim(); if (!transcript) return; const speakerVerified = await verifySpeaker(data); const emotion = await analyzeEmotion(data); const tags = []; if (speakerVerified) tags.push('[voice: verified]'); if (emotion) tags.push(`[emotion: ${emotion}]`); const taggedTranscript = `${tags.join(' ')} ${transcript}`; console.log(`?? ${taggedTranscript}`); await handleInput(taggedTranscript); } }); micInputStream.on('error', (err) => { console.error('?? Mic error:', err.message); }); micInputStream.on('startComplete', () => { console.log('?? Voice cognition pipeline activated (Vosk + SpeechBrain)'); }); // Auto-stop after 10 seconds for dev setTimeout(() => { console.log('?? Mic auto-stopping after 10 seconds...'); micInstance.stop(); }, 10000); micInstance.start(); [#// voice/voice-cognition.js import fs from 'fs'; import mic from 'mic'; import { model, kaldirecognizer } from 'vosk'; import { analyzeemotion, verifyspeaker } from '../utils/speechbrain.js'; import { handleinput } from '../scripts/soul-core.js'; // replace with correct path if needed const model_path = '/users/aegis/public/drop box/vosk-model-en-us-0.22'; const sample_rate = 16000; if (!fs.existssync(model_path)) { console.error(`? vosk model not found at #seg-1876]
