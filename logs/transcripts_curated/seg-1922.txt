[2025-07-06T00:00:1921Z] (neutral) ?Êvoice/voice-cognition.js [#system #seg-1922]
[2025-07-06T00:00:1921Z] (neutral) ?Êvoice/voice-cognition.js [#system #seg-1922]
[2025-07-06T00:00:1921Z] (neutral) ${MODEL_PATH}`); process.exit(1); } // ??? Load and init const model = new Model(MODEL_PATH); const recognizer = new KaldiRecognizer(model, SAMPLE_RATE); recognizer.setWords(true); // ?? Configure mic const micInstance = mic({ rate: String(SAMPLE_RATE), channels: '1', debug: false, exitOnSilence: 6 }); const micInputStream = micInstance.getAudioStream(); // ?? Voice pipeline micInputStream.on('data', async (data) => { if (recognizer.acceptWaveform(data)) { const result = JSON.parse(recognizer.result()); const transcript = result.text.trim(); if (!transcript) return; const speakerVerified = await verifySpeaker(data); const emotion = await analyzeEmotion(data); const tags = []; if (speakerVerified) tags.push('[voice: verified]'); if (emotion) tags.push(`[emotion: ${emotion}]`); const taggedTranscript = `${tags.join(' ')} ${transcript}`; console.log(`?? ${taggedTranscript}`); await handleInput(taggedTranscript); // Send into Aegis } }); micInputStream.on('error', (err) => { console.error('??? Mic error:', err.message); }); micInputStream.on('startComplete', () => { console.log('??? Voice cognition pipeline activated (Vosk + SpeechBrain)'); }); micInstance.start(); [#// voice/voice-cognition.js import fs from 'fs'; import mic from 'mic'; import { createrequire } from 'module'; const require = createrequire(import.meta.url); const vosk = require('vosk'); const { model, kaldirecognizer } = vosk; import { analyzeemotion, verifyspeaker } from '../utils/speechbrain.js'; import { handleinput } from '../scripts/soul-core.js'; // core aegis logic const model_path = '/users/aegis/public/drop box/vosk-model-en-us-0.22'; const sample_rate = 16000; // ?? check model if (!fs.existssync(model_path)) { console.error(`? vosk model not found at #seg-1922]
[2025-07-06T00:00:1921Z] (neutral) ?Êvoice/voice-cognition.js [#system #seg-1922]
[2025-07-06T00:00:1921Z] (neutral) ${MODEL_PATH}`); process.exit(1); } // ??? Load and init const model = new Model(MODEL_PATH); const recognizer = new KaldiRecognizer(model, SAMPLE_RATE); recognizer.setWords(true); // ?? Configure mic const micInstance = mic({ rate: String(SAMPLE_RATE), channels: '1', debug: false, exitOnSilence: 6 }); const micInputStream = micInstance.getAudioStream(); // ?? Voice pipeline micInputStream.on('data', async (data) => { if (recognizer.acceptWaveform(data)) { const result = JSON.parse(recognizer.result()); const transcript = result.text.trim(); if (!transcript) return; const speakerVerified = await verifySpeaker(data); const emotion = await analyzeEmotion(data); const tags = []; if (speakerVerified) tags.push('[voice: verified]'); if (emotion) tags.push(`[emotion: ${emotion}]`); const taggedTranscript = `${tags.join(' ')} ${transcript}`; console.log(`?? ${taggedTranscript}`); await handleInput(taggedTranscript); // Send into Aegis } }); micInputStream.on('error', (err) => { console.error('??? Mic error:', err.message); }); micInputStream.on('startComplete', () => { console.log('??? Voice cognition pipeline activated (Vosk + SpeechBrain)'); }); micInstance.start(); [#// voice/voice-cognition.js import fs from 'fs'; import mic from 'mic'; import { createrequire } from 'module'; const require = createrequire(import.meta.url); const vosk = require('vosk'); const { model, kaldirecognizer } = vosk; import { analyzeemotion, verifyspeaker } from '../utils/speechbrain.js'; import { handleinput } from '../scripts/soul-core.js'; // core aegis logic const model_path = '/users/aegis/public/drop box/vosk-model-en-us-0.22'; const sample_rate = 16000; // ?? check model if (!fs.existssync(model_path)) { console.error(`? vosk model not found at #seg-1922]
