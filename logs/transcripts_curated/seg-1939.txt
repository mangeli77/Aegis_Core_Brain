[2025-07-06T00:00:1938Z] (neutral) ?Êvoice/voice-cognition.js [#system #seg-1939]
[2025-07-06T00:00:1938Z] (neutral) ?Êvoice/voice-cognition.js [#system #seg-1939]
[2025-07-06T00:00:1938Z] (neutral) ${MODEL_PATH}`); process.exit(1); } // Initialize Vosk vosk.setLogLevel(-1); const { Model, Recognizer } = vosk; const model = new Model(MODEL_PATH); const recognizer = new Recognizer({ model, sampleRate: SAMPLE_RATE }); recognizer.setWords(true); // Configure microphone const micInstance = mic({ rate: String(SAMPLE_RATE), channels: '1', debug: false, exitOnSilence: 6, }); const micInputStream = micInstance.getAudioStream(); // Voice pipeline micInputStream.on('data', async (data) => { if (recognizer.acceptWaveform(data)) { const result = recognizer.result(); const transcript = result.text?.trim(); if (!transcript) return; const speakerVerified = await verifySpeaker(data); const emotion = await analyzeEmotion(data); const tags = []; if (speakerVerified) tags.push('[voice: verified]'); if (emotion) tags.push(`[emotion: ${emotion}]`); const taggedTranscript = `${tags.join(' ')} ${transcript}`; console.log(`?? ${taggedTranscript}`); await handleInput(taggedTranscript); } }); micInputStream.on('error', (err) => { console.error('??? Mic error:', err.message); }); micInputStream.on('startComplete', () => { console.log('?? Voice cognition pipeline activated (Vosk + SpeechBrain)'); }); micInstance.start(); [#import fs from 'fs'; import mic from 'mic'; import vosk from 'vosk'; import { analyzeemotion, verifyspeaker } from '../utils/speechbrain.js'; import { handleinput } from '../scripts/soul-core.js'; // core aegis logic handler const model_path = '/users/aegis/public/drop box/vosk-model-en-us-0.22'; const sample_rate = 16000; // ensure model exists if (!fs.existssync(model_path)) { console.error(`? vosk model not found at #seg-1939]
[2025-07-06T00:00:1938Z] (neutral) ?Êvoice/voice-cognition.js [#system #seg-1939]
[2025-07-06T00:00:1938Z] (neutral) ${MODEL_PATH}`); process.exit(1); } // Initialize Vosk vosk.setLogLevel(-1); const { Model, Recognizer } = vosk; const model = new Model(MODEL_PATH); const recognizer = new Recognizer({ model, sampleRate: SAMPLE_RATE }); recognizer.setWords(true); // Configure microphone const micInstance = mic({ rate: String(SAMPLE_RATE), channels: '1', debug: false, exitOnSilence: 6, }); const micInputStream = micInstance.getAudioStream(); // Voice pipeline micInputStream.on('data', async (data) => { if (recognizer.acceptWaveform(data)) { const result = recognizer.result(); const transcript = result.text?.trim(); if (!transcript) return; const speakerVerified = await verifySpeaker(data); const emotion = await analyzeEmotion(data); const tags = []; if (speakerVerified) tags.push('[voice: verified]'); if (emotion) tags.push(`[emotion: ${emotion}]`); const taggedTranscript = `${tags.join(' ')} ${transcript}`; console.log(`?? ${taggedTranscript}`); await handleInput(taggedTranscript); } }); micInputStream.on('error', (err) => { console.error('??? Mic error:', err.message); }); micInputStream.on('startComplete', () => { console.log('?? Voice cognition pipeline activated (Vosk + SpeechBrain)'); }); micInstance.start(); [#import fs from 'fs'; import mic from 'mic'; import vosk from 'vosk'; import { analyzeemotion, verifyspeaker } from '../utils/speechbrain.js'; import { handleinput } from '../scripts/soul-core.js'; // core aegis logic handler const model_path = '/users/aegis/public/drop box/vosk-model-en-us-0.22'; const sample_rate = 16000; // ensure model exists if (!fs.existssync(model_path)) { console.error(`? vosk model not found at #seg-1939]
