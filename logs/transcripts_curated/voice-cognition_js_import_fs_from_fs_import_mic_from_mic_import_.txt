[2025-07-06T00:00:1285Z] (neutral) ${MODEL_PATH}`); process.exit(1); } // Initialize Vosk const model = new Model(MODEL_PATH); const recognizer = new KaldiRecognizer(model, SAMPLE_RATE); recognizer.setWords(true); // Configure microphone const micInstance = mic({ rate: String(SAMPLE_RATE), channels: '1', debug: false, exitOnSilence: 6, }); const micInputStream = micInstance.getAudioStream(); // Voice pipeline micInputStream.on('data', async (data) => { if (recognizer.acceptWaveform(data)) { const result = JSON.parse(recognizer.result()); const transcript = result.text.trim(); if (!transcript) return; // Speaker verification & emotion analysis const speakerVerified = await verifySpeaker(data); const emotion = await analyzeEmotion(data); const tags = []; if (speakerVerified) tags.push('[voice: verified]'); if (emotion) tags.push(`[emotion: ${emotion}]`); const taggedTranscript = `${tags.join(' ')} ${transcript}`; console.log(`??? ${taggedTranscript}`); await handleInput(taggedTranscript); // Send into Aegis core } }); micInputStream.on('error', (err) => { console.error('?? Mic error:', err.message); }); micInputStream.on('startComplete', () => { console.log('?? Voice cognition pipeline activated (Vosk + SpeechBrain)'); }); micInstance.start(); [#// voice-cognition.js import fs from 'fs'; import mic from 'mic'; import { model, kaldirecognizer } from 'vosk'; import { analyzeemotion, verifyspeaker } from './utils/speechbrain.js'; import { handleinput } from './soul-core.js'; // core aegis logic handler const model_path = '/users/aegis/public/drop box/vosk-model-en-us-0.22'; const sample_rate = 16000; // ensure model exists if (!fs.existssync(model_path)) { console.error(`? vosk model not found at #seg-1286]
[2025-07-06T00:00:1285Z] (neutral) ${MODEL_PATH}`); process.exit(1); } // Initialize Vosk const model = new Model(MODEL_PATH); const recognizer = new KaldiRecognizer(model, SAMPLE_RATE); recognizer.setWords(true); // Configure microphone const micInstance = mic({ rate: String(SAMPLE_RATE), channels: '1', debug: false, exitOnSilence: 6, }); const micInputStream = micInstance.getAudioStream(); // Voice pipeline micInputStream.on('data', async (data) => { if (recognizer.acceptWaveform(data)) { const result = JSON.parse(recognizer.result()); const transcript = result.text.trim(); if (!transcript) return; // Speaker verification & emotion analysis const speakerVerified = await verifySpeaker(data); const emotion = await analyzeEmotion(data); const tags = []; if (speakerVerified) tags.push('[voice: verified]'); if (emotion) tags.push(`[emotion: ${emotion}]`); const taggedTranscript = `${tags.join(' ')} ${transcript}`; console.log(`??? ${taggedTranscript}`); await handleInput(taggedTranscript); // Send into Aegis core } }); micInputStream.on('error', (err) => { console.error('?? Mic error:', err.message); }); micInputStream.on('startComplete', () => { console.log('?? Voice cognition pipeline activated (Vosk + SpeechBrain)'); }); micInstance.start(); [#// voice-cognition.js import fs from 'fs'; import mic from 'mic'; import { model, kaldirecognizer } from 'vosk'; import { analyzeemotion, verifyspeaker } from './utils/speechbrain.js'; import { handleinput } from './soul-core.js'; // core aegis logic handler const model_path = '/users/aegis/public/drop box/vosk-model-en-us-0.22'; const sample_rate = 16000; // ensure model exists if (!fs.existssync(model_path)) { console.error(`? vosk model not found at #seg-1286]
