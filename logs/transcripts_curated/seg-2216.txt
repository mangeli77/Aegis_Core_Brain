[2025-07-06T00:00:2215Z] (neutral) voice/voice-cognition.jsÊ(Save this) [#system #seg-2216]
[2025-07-06T00:00:2215Z] (neutral) voice/voice-cognition.jsÊ(Save this) [#system #seg-2216]
[2025-07-06T00:00:2215Z] (neutral) ${MODEL_PATH}`); process.exit(1); } vosk.setLogLevel(0); const { Model, Recognizer } = vosk; const model = new Model(MODEL_PATH); const recognizer = new Recognizer({ model, sampleRate: SAMPLE_RATE }); recognizer.setWords(true); const micInstance = mic({ rate: String(SAMPLE_RATE), channels: '1', debug: false, exitOnSilence: 6, }); const micInputStream = micInstance.getAudioStream(); micInputStream.on('data', async (data) => { if (recognizer.acceptWaveform(data)) { const result = JSON.parse(recognizer.result()); const transcript = result.text.trim(); if (!transcript) return; const speakerVerified = await verifySpeaker(data); const emotion = await analyzeEmotion(data); const tags = []; if (speakerVerified) tags.push('[voice: verified]'); if (emotion) tags.push(`[emotion: ${emotion}]`); const taggedTranscript = `${tags.join(' ')} ${transcript}`; console.log(`?? ${taggedTranscript}`); await handleInput(taggedTranscript); } }); micInputStream.on('error', (err) => { console.error('??? Mic error:', err.message); }); micInputStream.on('startComplete', () => { console.log('?? Voice cognition pipeline activated (Vosk + SpeechBrain)'); }); micInstance.start(); [#import fs from 'fs'; import mic from 'mic'; import vosk from 'vosk'; import { analyzeemotion, verifyspeaker } from '../utils/speechbrain.js'; import { handleinput } from '../scripts/soul-core.js'; const model_path = "/users/aegis/public/drop box/vosk-model-en-us-0.22"; const sample_rate = 16000; if (!fs.existssync(model_path)) { console.error(`? vosk model not found at #seg-2216]
[2025-07-06T00:00:2215Z] (neutral) voice/voice-cognition.jsÊ(Save this) [#system #seg-2216]
[2025-07-06T00:00:2215Z] (neutral) ${MODEL_PATH}`); process.exit(1); } vosk.setLogLevel(0); const { Model, Recognizer } = vosk; const model = new Model(MODEL_PATH); const recognizer = new Recognizer({ model, sampleRate: SAMPLE_RATE }); recognizer.setWords(true); const micInstance = mic({ rate: String(SAMPLE_RATE), channels: '1', debug: false, exitOnSilence: 6, }); const micInputStream = micInstance.getAudioStream(); micInputStream.on('data', async (data) => { if (recognizer.acceptWaveform(data)) { const result = JSON.parse(recognizer.result()); const transcript = result.text.trim(); if (!transcript) return; const speakerVerified = await verifySpeaker(data); const emotion = await analyzeEmotion(data); const tags = []; if (speakerVerified) tags.push('[voice: verified]'); if (emotion) tags.push(`[emotion: ${emotion}]`); const taggedTranscript = `${tags.join(' ')} ${transcript}`; console.log(`?? ${taggedTranscript}`); await handleInput(taggedTranscript); } }); micInputStream.on('error', (err) => { console.error('??? Mic error:', err.message); }); micInputStream.on('startComplete', () => { console.log('?? Voice cognition pipeline activated (Vosk + SpeechBrain)'); }); micInstance.start(); [#import fs from 'fs'; import mic from 'mic'; import vosk from 'vosk'; import { analyzeemotion, verifyspeaker } from '../utils/speechbrain.js'; import { handleinput } from '../scripts/soul-core.js'; const model_path = "/users/aegis/public/drop box/vosk-model-en-us-0.22"; const sample_rate = 16000; if (!fs.existssync(model_path)) { console.error(`? vosk model not found at #seg-2216]
